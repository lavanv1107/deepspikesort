{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of SpikeInterface as recommended in the **From source** section [here](https://spikeinterface.readthedocs.io/en/latest/get_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "from one.api import ONE\n",
    "\n",
    "print(f\"SpikeInterface version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "import preprocessing\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read recording session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will be using session [sub-CSHL049](https://dandiarchive.org/dandiset/000409/draft/files?location=sub-CSHL049&page=1) of the [IBL Brain Wide Map Dataset](https://dandiarchive.org/dandiset/000409/draft). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data/sub-CSHL049\"\n",
    "\n",
    "os.makedirs(data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SpikeInterface, we can read and save the data to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extractors_folder = os.path.join(data_folder, \"extractors\")\n",
    "\n",
    "os.makedirs(extractors_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain this data, we will stream with ONE API using its identifier which is listed in the [metadata](https://api.dandiarchive.org/api/dandisets/000409/versions/draft/assets/7e4fa468-349c-44a9-a482-26898682eed1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one = ONE(base_url=\"https://openalyx.internationalbrainlab.org\", password=\"international\", silent=True)\n",
    "\n",
    "eid = \"c99d53e6-c317-4c53-99ba-070b26673ac4\"\n",
    "pids, _ = one.eid2pid(eid)\n",
    "pid = pids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_folder = os.path.join(data_folder, \"one\")\n",
    "\n",
    "os.makedirs(one_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_folder = os.path.join(extractors_folder, \"preprocessed\")\n",
    "\n",
    "if not os.path.exists(preprocessed_folder): \n",
    "    recording = si.read_ibl_recording(eid, pid, 'probe00.ap', cache_folder=one_folder)\n",
    "    \n",
    "    # Preprocess the recording\n",
    "    recording_f = si.bandpass_filter(recording, freq_min=300, freq_max=6000)\n",
    "    recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "    \n",
    "    # Save the preprocessed recording to disk\n",
    "    job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)\n",
    "    recording_cmr.save(folder=preprocessed_folder, **job_kwargs)\n",
    "else:\n",
    "    recording_cmr = si.load_extractor(preprocessed_folder)\n",
    "    \n",
    "recording_cmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_folder = os.path.join(extractors_folder, \"sorting\")\n",
    "\n",
    "if not os.path.exists(sorting_folder):  \n",
    "    sorting = si.read_ibl_sorting(pid)        \n",
    "    sorting.save(folder=sorting_folder)\n",
    "else:\n",
    "    sorting = si.load_extractor(sorting_folder)\n",
    "    \n",
    "sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sorting Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer_folder = os.path.join(extractors_folder, \"analyzer\")\n",
    "\n",
    "if not os.path.exists(analyzer_folder):\n",
    "    analyzer = si.create_sorting_analyzer(\n",
    "        sorting=sorting,\n",
    "        recording=recording_cmr,\n",
    "        format=\"memory\"\n",
    "    )\n",
    "    \n",
    "    # Compute extensions\n",
    "    job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)\n",
    "    compute_dict = {\n",
    "        'random_spikes': {'method': 'uniform'},\n",
    "        'waveforms': {'ms_before': 1.0, 'ms_after': 2.0},\n",
    "        'templates': {'operators': [\"average\", \"median\", \"std\"]}\n",
    "    }\n",
    "    analyzer.compute(compute_dict, **job_kwargs)\n",
    "    \n",
    "    # Save the sorting analyzer to disk\n",
    "    analyzer.save_as(folder=analyzer_folder, format=\"binary_folder\")\n",
    "else:\n",
    "    analyzer = si.load_sorting_analyzer(analyzer_folder)\n",
    "    \n",
    "analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract the spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channels_file = os.path.join(extractors_folder, \"channels.npy\")\n",
    "\n",
    "if not os.path.exists(spikes_file):\n",
    "    channels = preprocessing.extract_channels(recording_cmr)\n",
    "    np.save(channels_file, channels)\n",
    "else:\n",
    "    channels = np.load(channels_file)\n",
    "\n",
    "display(pd.DataFrame(channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes_folder = os.path.join(data_folder, \"spikes\")\n",
    "\n",
    "os.makedirs(spikes_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spike_index</th>\n",
       "      <th>sample_index</th>\n",
       "      <th>channel_index</th>\n",
       "      <th>channel_location_x</th>\n",
       "      <th>channel_location_y</th>\n",
       "      <th>unit_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>341</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>361</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>606</td>\n",
       "      <td>354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3540.0</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>680</td>\n",
       "      <td>361</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>715</td>\n",
       "      <td>325</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3240.0</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4604408</th>\n",
       "      <td>4604408</td>\n",
       "      <td>125188816</td>\n",
       "      <td>21</td>\n",
       "      <td>48.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4604409</th>\n",
       "      <td>4604409</td>\n",
       "      <td>125188838</td>\n",
       "      <td>155</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4604410</th>\n",
       "      <td>4604410</td>\n",
       "      <td>125188912</td>\n",
       "      <td>325</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3240.0</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4604411</th>\n",
       "      <td>4604411</td>\n",
       "      <td>125188967</td>\n",
       "      <td>326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3260.0</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4604412</th>\n",
       "      <td>4604412</td>\n",
       "      <td>125189064</td>\n",
       "      <td>157</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1560.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4604413 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         spike_index  sample_index  channel_index  channel_location_x  \\\n",
       "0                  0           472            341                48.0   \n",
       "1                  1           511            361                48.0   \n",
       "2                  2           606            354                 0.0   \n",
       "3                  3           680            361                48.0   \n",
       "4                  4           715            325                48.0   \n",
       "...              ...           ...            ...                 ...   \n",
       "4604408      4604408     125188816             21                48.0   \n",
       "4604409      4604409     125188838            155                32.0   \n",
       "4604410      4604410     125188912            325                48.0   \n",
       "4604411      4604411     125188967            326                 0.0   \n",
       "4604412      4604412     125189064            157                48.0   \n",
       "\n",
       "         channel_location_y  unit_index  \n",
       "0                    3400.0         271  \n",
       "1                    3600.0         306  \n",
       "2                    3540.0         297  \n",
       "3                    3600.0         306  \n",
       "4                    3240.0         235  \n",
       "...                     ...         ...  \n",
       "4604408               200.0          26  \n",
       "4604409              1540.0         105  \n",
       "4604410              3240.0         237  \n",
       "4604411              3260.0         239  \n",
       "4604412              1560.0         106  \n",
       "\n",
       "[4604413 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spikes_file = os.path.join(spikes_folder, \"spikes.npy\")\n",
    "\n",
    "if not os.path.exists(spikes_file):\n",
    "    spikes = preprocessing.extract_spikes(sorting, analyzer, channels)\n",
    "    np.save(spikes_file, spikes)\n",
    "else:\n",
    "    spikes = np.load(spikes_file)\n",
    "    \n",
    "display(pd.DataFrame(spikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_file = os.path.join(spikes_folder, \"noise.npy\")\n",
    "\n",
    "if not os.path.exists(noise_file):\n",
    "    noise = preprocessing.create_noise(recording_cmr, spikes, num_samples=100000)\n",
    "    np.save(noise_file, noise)\n",
    "else:\n",
    "    noise = np.load(noise_file)\n",
    "    \n",
    "display(pd.DataFrame(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create trace dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spike_units = spikes['unit_index']\n",
    "\n",
    "print(util.format_value_counts(spike_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset of HDF5 files using `submit_dataset.sh`. \n",
    "\n",
    "You will need to specify 2 arguments:\n",
    "- [1] The ID of the recording\n",
    "- [2] The type of dataset: 'spikes', 'peaks', or 'noise'\n",
    "\n",
    "\n",
    "Each file belongs to an identified unit within the spikes from the NWB file. Within each file are two datasets:\n",
    "- A dataset of frame numbers for when each sample occurred\n",
    "- A dataset of traces for each sample belonging to the unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classify with CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run classification on spikes using `run_classify.py` from the main folder (or `submit_run_classify.sh` with SLURM).\n",
    "\n",
    "You will need to specify 8 arguments:\n",
    "- [1] The ID of the recording\n",
    "- [2] The minimum number of samples per unit\n",
    "- [3] The maximum number of samples per unit ('max' for the max number of samples per unit)\n",
    "- [4] The number of units to be classified\n",
    "- [5] The number of samples to be used per unit ('all' to use all samples)\n",
    "- [6] The number of noise samples to include (0 for none, 'all' for all)\n",
    "- [7] The name to set the session ID\n",
    "- [8] The number to set the session ID\n",
    "\n",
    "Example: `python -m phase_1.run_classify sub-CSHL049 1000 5000 3 all 0 sup 0`\n",
    "\n",
    "The example command will run:\n",
    "- using recording sub-CSHL049\n",
    "- on 3 units\n",
    "- with 1000-5000 samples per unit\n",
    "- using all samples per unit\n",
    "- including 0 noise samples\n",
    "- named session SUP_000\n",
    "\n",
    "The script will also save the classification results to the results folder:\n",
    "- Accuracy and Loss plot\n",
    "- Accuracy and Loss log\n",
    "- Confusion matrix "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
