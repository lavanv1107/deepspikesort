{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of SpikeInterface as recommended in the **From source** section [here](https://spikeinterface.readthedocs.io/en/latest/get_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "from one.api import ONE\n",
    "\n",
    "print(f\"SpikeInterface version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "import preprocessing\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read recording session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will be using session [sub-CSHL049](https://dandiarchive.org/dandiset/000409/draft/files?location=sub-CSHL049&page=1) of the [IBL Brain Wide Map Dataset](https://dandiarchive.org/dandiset/000409/draft). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data/sub-CSHL049\"\n",
    "\n",
    "os.makedirs(data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SpikeInterface, we can read and save the data to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extractors_folder = os.path.join(data_folder, \"extractors\")\n",
    "\n",
    "os.makedirs(extractors_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain this data, we will stream with ONE API using its identifier which is listed in the [metadata](https://api.dandiarchive.org/api/dandisets/000409/versions/draft/assets/7e4fa468-349c-44a9-a482-26898682eed1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one = ONE(base_url=\"https://openalyx.internationalbrainlab.org\", password=\"international\", silent=True)\n",
    "\n",
    "eid = \"c99d53e6-c317-4c53-99ba-070b26673ac4\"\n",
    "pids, _ = one.eid2pid(eid)\n",
    "pid = pids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_folder = os.path.join(data_folder, \"one\")\n",
    "\n",
    "os.makedirs(one_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_folder = os.path.join(extractors_folder, \"preprocessed\")\n",
    "\n",
    "if not os.path.exists(preprocessed_folder): \n",
    "    recording = si.read_ibl_recording(eid, pid, 'probe00.ap', cache_folder=one_folder)\n",
    "    \n",
    "    # Preprocess the recording\n",
    "    recording_f = si.bandpass_filter(recording, freq_min=300, freq_max=6000)\n",
    "    recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "    \n",
    "    # Save the preprocessed recording to disk\n",
    "    job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)\n",
    "    recording_cmr.save(folder=preprocessed_folder, **job_kwargs)\n",
    "else:\n",
    "    recording_cmr = si.load_extractor(preprocessed_folder)\n",
    "    \n",
    "recording_cmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_folder = os.path.join(extractors_folder, \"sorting\")\n",
    "\n",
    "if not os.path.exists(sorting_folder):  \n",
    "    sorting = si.read_ibl_sorting(pid)        \n",
    "    sorting.save(folder=sorting_folder)\n",
    "else:\n",
    "    sorting = si.load_extractor(sorting_folder)\n",
    "    \n",
    "sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sorting Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyzer_folder = os.path.join(extractors_folder, \"analyzer\")\n",
    "\n",
    "if not os.path.exists(analyzer_folder):\n",
    "    analyzer = si.create_sorting_analyzer(\n",
    "        sorting=sorting,\n",
    "        recording=recording_cmr,\n",
    "        format=\"memory\"\n",
    "    )\n",
    "    \n",
    "    # Compute extensions\n",
    "    job_kwargs = dict(n_jobs=8, chunk_duration=\"1s\", progress_bar=True)\n",
    "    compute_dict = {\n",
    "        'random_spikes': {'method': 'uniform'},\n",
    "        'waveforms': {'ms_before': 1.0, 'ms_after': 2.0},\n",
    "        'templates': {'operators': [\"average\", \"median\", \"std\"]}\n",
    "    }\n",
    "    analyzer.compute(compute_dict, **job_kwargs)\n",
    "    \n",
    "    # Save the sorting analyzer to disk\n",
    "    analyzer.save_as(folder=analyzer_folder, format=\"binary_folder\")\n",
    "else:\n",
    "    analyzer = si.load_sorting_analyzer(analyzer_folder)\n",
    "    \n",
    "analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract the spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes_folder = os.path.join(data_folder, \"spikes\")\n",
    "\n",
    "os.makedirs(spikes_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes_file = os.path.join(spikes_folder, \"spikes.npy\")\n",
    "\n",
    "if not os.path.exists(spikes_file):\n",
    "    spikes = preprocessing.extract_spikes(sorting, analyzer)\n",
    "    np.save(spikes_file, spikes)\n",
    "else:\n",
    "    spikes = np.load(spikes_file)\n",
    "    \n",
    "display(pd.DataFrame(spikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_file = os.path.join(spikes_folder, \"noise.npy\")\n",
    "\n",
    "if not os.path.exists(noise_file):\n",
    "    noise = preprocessing.create_noise(recording_cmr, spikes, num_samples=100000)\n",
    "    np.save(noise_file, noise)\n",
    "else:\n",
    "    noise = np.load(noise_file)\n",
    "    \n",
    "display(pd.DataFrame(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create trace dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spike_units = spikes['unit_index']\n",
    "\n",
    "print(util.format_value_counts(spike_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset of HDF5 files using `submit_dataset.sh`. \n",
    "\n",
    "You will need to specify 2 arguments:\n",
    "- [1] The ID of the recording\n",
    "- [2] The type of dataset: 'spikes', 'peaks', or 'noise'\n",
    "\n",
    "\n",
    "Each file belongs to an identified unit within the spikes from the NWB file. Within each file are two datasets:\n",
    "- A dataset of frame numbers for when each sample occurred\n",
    "- A dataset of traces for each sample belonging to the unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classify with CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run classification on spikes using `run_classify.py` from the main folder (or `submit_run_classify.sh` with SLURM).\n",
    "\n",
    "You will need to specify 8 arguments:\n",
    "- [1] The ID of the recording\n",
    "- [2] The minimum number of samples per unit\n",
    "- [3] The maximum number of samples per unit ('max' for the max number of samples per unit)\n",
    "- [4] The number of units to be classified\n",
    "- [5] The number of samples to be used per unit ('all' to use all samples)\n",
    "- [6] The number of noise samples to include (0 for none, 'all' for all)\n",
    "- [7] The name to set the session ID\n",
    "- [8] The number to set the session ID\n",
    "\n",
    "Example: `python -m phase_1.run_classify sub-CSHL049 1000 5000 3 all 0 sup 0`\n",
    "\n",
    "The example command will run:\n",
    "- using recording sub-CSHL049\n",
    "- on 3 units\n",
    "- with 1000-5000 samples per unit\n",
    "- using all samples per unit\n",
    "- including 0 noise samples\n",
    "- named session SUP_000\n",
    "\n",
    "The script will also save the classification results to the results folder:\n",
    "- Accuracy and Loss plot\n",
    "- Accuracy and Loss log\n",
    "- Confusion matrix "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rapids",
   "language": "python",
   "name": "rapids-24.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
