{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# This notebook creates a dataset from a session of the IBL Brain Wide Map Dataset\n",
    "\n",
    "You will need to install the following packages:\n",
    "- numpy\n",
    "- pandas\n",
    "- spikeinterface\n",
    "- ONE-api\n",
    "- ibllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from one.api import ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of SpikeInterface as recommended in the **From source** section [here](https://spikeinterface.readthedocs.io/en/latest/get_started/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "from spikeinterface.sortingcomponents.peak_detection import detect_peaks\n",
    "\n",
    "print(f\"SpikeInterface version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install local functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import process_peaks\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import preprocessing\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read recording session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will be using session [sub-CSHL049](https://dandiarchive.org/dandiset/000409/draft/files?location=sub-CSHL049&page=1) of the [IBL Brain Wide Map Dataset](https://dandiarchive.org/dandiset/000409/draft). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data/sub-CSHL049\"\n",
    "\n",
    "os.makedirs(data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain this data, we will stream with ONE API using its identifier which is listed in the [metadata](https://api.dandiarchive.org/api/dandisets/000409/versions/draft/assets/7e4fa468-349c-44a9-a482-26898682eed1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_folder = os.path.join(data_folder, \"one\")\n",
    "\n",
    "os.makedirs(one_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one = ONE(\n",
    "    base_url=\"https://openalyx.internationalbrainlab.org\",\n",
    "    username=\"intbrainlab\",  \n",
    "    password=\"international\",\n",
    "    silent=True\n",
    ")\n",
    "\n",
    "eid = \"c99d53e6-c317-4c53-99ba-070b26673ac4\"\n",
    "pids, _ = one.eid2pid(eid)\n",
    "pid = pids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SpikeInterface, we can read and save the data to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extractors_folder = os.path.join(data_folder, \"extractors\")\n",
    "\n",
    "os.makedirs(extractors_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_folder = os.path.join(extractors_folder, \"preprocessed\")\n",
    "\n",
    "if not os.path.exists(preprocessed_folder): \n",
    "    recording = se.read_ibl_recording(eid=eid, stream_name='probe00.ap', cache_folder=one_folder)\n",
    "    \n",
    "    # Preprocess the recording\n",
    "    recording_f = spre.bandpass_filter(recording, freq_min=300, freq_max=6000)\n",
    "    recording_cmr = spre.common_reference(recording_f, reference='global', operator='median')\n",
    "    \n",
    "    # Save the preprocessed recording to disk\n",
    "    job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)\n",
    "    recording_cmr.save(folder=preprocessed_folder, **job_kwargs)\n",
    "else:\n",
    "    recording_cmr = si.load_extractor(preprocessed_folder)\n",
    "    \n",
    "recording_cmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create peaks dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve channel locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_locations_file = os.path.join(data_folder, \"channel_locations.npy\")\n",
    "\n",
    "if not os.path.exists(channel_locations_file):\n",
    "    channel_locations = preprocessing.extract_channels(recording_cmr)\n",
    "    np.save(channel_locations_file, channel_locations)\n",
    "else:\n",
    "    channel_locations = np.load(channel_locations_file)\n",
    "\n",
    "display(pd.DataFrame(channel_locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_folder = '../data/sub-CSHL049/peaks'\n",
    "peaks_file = os.path.join(peaks_folder, \"peaks.npy\")\n",
    "\n",
    "if os.path.exists(peaks_file):\n",
    "    peaks_filtered = np.load(peaks_file)\n",
    "else:\n",
    "    os.makedirs(peaks_folder, exist_ok=True)\n",
    "    \n",
    "    job_kwargs = dict(chunk_duration='1s', n_jobs=10, progress_bar=True)\n",
    "    \n",
    "    peaks = detect_peaks(\n",
    "        recording_cmr,\n",
    "        method='locally_exclusive',\n",
    "        peak_sign='neg',\n",
    "        detect_threshold=6,\n",
    "        radius_um = 100,\n",
    "        **job_kwargs\n",
    "    )    \n",
    "    \n",
    "    peaks_filtered = process_peaks.filter_peaks(recording_cmr, peaks)\n",
    "    \n",
    "    np.save(peaks_file, peaks_filtered)\n",
    "    \n",
    "display(pd.DataFrame(peaks_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create HDF5 files each containing peaks data using `create_peaks_files.sh`. \n",
    "You will need to specify the ID of the recording (e.g., 'sub-CSHL049'). This creates multiple HDF5 files based on the number of tasks you set in the jobscript.\n",
    "\n",
    "Next, run the `combine_peaks_files.py` file to combine the HDF5 files.\n",
    "You will need to specify the ID of the recording (e.g., 'sub-CSHL049'). This creates a single HDF5 file containing the complete peaks dataset.\n",
    "\n",
    "Within the file are three datasets:\n",
    "- channel_locations [384, 3]: A dataset of channel indices and their corresponding locations on the probe.\n",
    "- properties [n, 3]: A dataset of different properties for each peak - sample_index, channel_index, and amplitude.\n",
    "- traces [n, 64, 192, 2]: A dataset of traces for each peak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
