{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Phase 1: Proof of Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this initial trial is to investigate if it would be possible to train a model to learn neuronal spiking activity. A large part of this process is to first unpack and understand the data we are working with in order to process it as inputs. We also implement different neural network architectures to test out their effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore the data in an NWB file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are readily available ground-truth datasets in NWB files which contain spikes that have been manually curated by experts. We are going to use the `sub-CSHL049_ses-c99d53e6-c317-4c53-99ba-070b26673ac4_behavior+ecephys+image.nwb` file which can be downloaded from the DANDI archive:\n",
    "https://api.dandiarchive.org/api/assets/7e4fa468-349c-44a9-a482-26898682eed1/download/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We followed the instructions for using `SpikeInterface` based on this tutorial:\n",
    "https://github.com/SpikeInterface/spiketutorials/tree/master/Official_Tutorial_SI_0.96_Oct22 \n",
    "\n",
    "Install the latest version of `SpikeInterface` from source as recommended in the **\"From source\"** section here: \n",
    "https://spikeinterface.readthedocs.io/en/latest/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "\n",
    "print(f\"SpikeInterface version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import plotting\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read the NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nwb_file = \"data/sub-CSHL049_ses-c99d53e6-c317-4c53-99ba-070b26673ac4_behavior+ecephys+image.nwb\"\n",
    "\n",
    "recording_nwb = si.read_nwb(nwb_file, electrical_series_name='ElectricalSeriesAp')\n",
    "recording_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_nwb.annotate(is_filtered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_nwb = si.read_nwb_sorting(file_path=nwb_file, electrical_series_name='ElectricalSeriesAp')\n",
    "sorting_nwb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_f = si.bandpass_filter(recording_nwb, freq_min=300, freq_max=6000)\n",
    "recording_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "recording_cmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_slice = preprocessing.channel_slice_electricalseriesap(recording_cmr)\n",
    "recording_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save extractors to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extractors_folder = \"extractors/001\"\n",
    "\n",
    "os.makedirs(extractors_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_folder = os.path.join(extractors_folder, \"preprocessed\")\n",
    "job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)\n",
    "\n",
    "if not os.path.exists(preprocessed_folder):    \n",
    "    recording_slice.save(folder=preprocessed_folder, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_folder = os.path.join(extractors_folder, \"sorting\")\n",
    "\n",
    "if not os.path.exists(sorting_folder):    \n",
    "    sorting_nwb.save(folder=sorting_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect channels on probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channels = preprocessing.extract_channels(recording_slice)\n",
    "    \n",
    "display(pd.DataFrame(channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_probe_map(recording_slice, with_channel_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect spike events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using an NWB file that contains both the raw recording and spike sorted data, we can extract information of the already sorted spikes.\n",
    "\n",
    "We need these expert-sorted spikes in order to determine the best channels and frames for plotting our images and labelling them as spikes for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to retrieve information about these spikes, we need to create a `WaveformExtractor` object which has mechanisms provided by `SpikeInterface` for computing the spike locations as well as plotting them on the probe.\n",
    "\n",
    "A `WaveformExtractor` object requires a paired `Recording` and `Sorting object` which we already have.\n",
    "\n",
    "More information on waveform extractors can be found here:\n",
    "https://spikeinterface.readthedocs.io/en/latest/modules_gallery/core/plot_4_waveform_extractor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "waveform_folder = os.path.join(extractors_folder, \"waveform\")\n",
    "\n",
    "if os.path.exists(waveform_folder):\n",
    "    waveform_nwb = si.load_waveforms(waveform_folder, with_recording=False)\n",
    "else:\n",
    "    job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)\n",
    "\n",
    "    waveform_nwb = si.extract_waveforms(\n",
    "        recording_slice,\n",
    "        sorting_nwb,\n",
    "        waveform_folder,\n",
    "        ms_before=1.5,\n",
    "        ms_after=2.,\n",
    "        max_spikes_per_unit=None,\n",
    "        overwrite=True,\n",
    "        **job_kwargs\n",
    "    )\n",
    "    \n",
    "waveform_nwb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the frames each spike occurred (since `SpikeInterface` uses frames instead of seconds) by using the `get_all_spike_trains()` function which returns a list containing two arrays including each spike's unit ID and frame.\n",
    "\n",
    "Each individual spike frame is the rounded product of its corresponding spike time and the sampling frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes_folder = 'spikes/001'\n",
    "\n",
    "os.makedirs(spikes_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes = preprocessing.extract_spikes(sorting_nwb, waveform_nwb) \n",
    "\n",
    "spikes_file = os.path.join(spikes_folder, \"spikes_nwb.npy\")\n",
    "\n",
    "if not os.path.exists(spikes_file):\n",
    "    np.save(spikes_file, spikes)\n",
    "    \n",
    "display(pd.DataFrame(spikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_unit_waveform(recording_slice, spikes, unit_id=5, num_waveforms=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of how the channels on a Neuropixels probe are arranged in a checkerboard pattern, we want to reshape our trace to better emulate that. This would mean separating the channels into two columns resulting in a 3-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_trace_image(recording_slice, 471)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Create a dataset from sorted spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset of HDF5 files can be generated using the `generate_dataset.py` script. \n",
    "\n",
    "Each file belongs to an identified unit within the spikes from the NWB file. Within each file are two datasets:\n",
    "- A dataset of frame numbers for when each sample occurred\n",
    "- A dataset of trace representations of each sample belonging to the unit\n",
    "\n",
    "The script can also generate an HDF5 file of noise samples (named 'unit_-01') based on a number you specify (up to 1 million). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spike_units = spikes['unit_index']\n",
    "\n",
    "print(f'Spike units: {len(np.unique(spike_units))}\\n')\n",
    "print(util.format_value_counts(spike_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script needs to be run with 4 arguments:\n",
    "- [1] The number associated with the recording to be used\n",
    "- [2] The type of dataset to be generated - 0 for spikes from the NWB file, 2 for noise samples\n",
    "\n",
    "Example: `!python generate_dataset.py 1 0 0 423`\n",
    "\n",
    "This example command will generate a dataset of spikes from recording number 1 starting from unit 0 to 423."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classify spikes and noise with a CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier model can be run using the run_classifier.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script needs to be run with 8 arguments:\n",
    "\n",
    "- [1] The number associated with the recording to be used\n",
    "- [2] The minimum number of samples per unit\n",
    "- [3] The maximum number of samples per unit \n",
    "- [4] The number of units to be classified\n",
    "- [5] The number of noise samples to include in the training data\n",
    "- [6] The number of available GPUs for parallel data loading\n",
    "- [7] The number of epochs for running DSS\n",
    "- [8] The number to set the session ID\n",
    "\n",
    "Example: `!python run_classifier.py 1 1000 2000 10 1000 1 200 1`\n",
    "\n",
    "The example command will run DSS:\n",
    "- named session 001\n",
    "- for 200 epochs \n",
    "- on 10 units\n",
    "- each with 1000-2000 samples\n",
    "- including 1000 noise samples\n",
    "- using 1 available GPU\n",
    "\n",
    "The script will also save the classification results to the results folder:\n",
    "- Accuracy progress plot\n",
    "- Accuracy and Loss progress log\n",
    "- Confusion matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSpikeSort",
   "language": "python",
   "name": "deep_spike_sort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
