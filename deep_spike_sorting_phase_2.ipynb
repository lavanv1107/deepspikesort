{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: A Semi-Supervised Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract the data in an NWB file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import psutil\n",
    "import multiprocessing as mp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import torchvision\n",
    "from torchvision import utils\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se \n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.widgets as sw\n",
    "from spikeinterface.sortingcomponents.peak_detection import detect_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"SpikeInterface version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import deepspikesort as dss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_folder = Path(\".\")\n",
    "nwb_file = \"sub-CSHL049_ses-c99d53e6-c317-4c53-99ba-070b26673ac4_behavior+ecephys+image.nwb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_nwb = se.read_nwb_recording(file_path=nwb_file, electrical_series_name='ElectricalSeriesAp')\n",
    "recording_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_nwb.annotate(is_filtered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_nwb = se.read_nwb_sorting(file_path=nwb_file, electrical_series_name='ElectricalSeriesAp')\n",
    "sorting_nwb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_f = spre.bandpass_filter(recording_nwb, freq_min=300, freq_max=6000)\n",
    "recording_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_cmr = spre.common_reference(recording_f, reference='global', operator='median')\n",
    "recording_cmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract channels and spikes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_slice = dss.channel_slice_electricalseriesap(recording_cmr)\n",
    "recording_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channels_table = dss.extract_channels(recording_slice)\n",
    "display(channels_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_table['channel_loc_x'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waveform_folder = 'waveform'\n",
    "\n",
    "job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (base_folder / waveform_folder).is_dir():\n",
    "    waveform_nwb = si.load_waveforms(base_folder / waveform_folder)\n",
    "else:\n",
    "    waveform_nwb = si.extract_waveforms(\n",
    "        recording_slice,\n",
    "        sorting_nwb,\n",
    "        waveform_folder,\n",
    "        ms_before=1.5,\n",
    "        ms_after=2.,\n",
    "        max_spikes_per_unit=None,\n",
    "        overwrite=True,\n",
    "        **job_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waveform_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes_table = dss.extract_spikes(sorting_nwb, waveform_nwb)\n",
    "display(spikes_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Match peaks to spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_folder = 'peaks'\n",
    "\n",
    "job_kwargs = dict(chunk_duration='1s', n_jobs=8, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (base_folder / peaks_folder).is_dir():\n",
    "    peaks = dss.load_peaks(base_folder / peaks_folder)\n",
    "else:\n",
    "    peaks = detect_peaks(recording_slice,\n",
    "                         method='locally_exclusive',\n",
    "                         peak_sign='neg',\n",
    "                         detect_threshold=6,\n",
    "                         **job_kwargs\n",
    "                        )\n",
    "    dss.save_peaks(peaks, base_folder / peaks_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_table = dss.extract_peaks(recording_slice, peaks)\n",
    "display(peaks_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_matched_table = dss.match_peaks(peaks_table, spikes_table, channels_table)\n",
    "display(peaks_matched_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_spikes_table = dss.get_peaks_spikes(peaks_matched_table)\n",
    "peaks_noise_table = dss.get_peaks_noise(peaks_matched_table)\n",
    "\n",
    "display(peaks_spikes_table)\n",
    "display(peaks_noise_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a dataset from matched peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nthreads = psutil.cpu_count(logical=True)\n",
    "ncores = psutil.cpu_count(logical=False)\n",
    "nthreads_per_core = nthreads // ncores\n",
    "nthreads_available = len(os.sched_getaffinity(0))\n",
    "ncores_available = nthreads_available // nthreads_per_core\n",
    "\n",
    "assert nthreads == os.cpu_count()\n",
    "assert nthreads == mp.cpu_count()\n",
    "\n",
    "print(f'{nthreads=}')\n",
    "print(f'{ncores=}')\n",
    "print(f'{nthreads_per_core=}')\n",
    "print(f'{nthreads_available=}')\n",
    "print(f'{ncores_available=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_dataset_folder = os.path.join(peaks_folder, 'peaks_dataset')\n",
    "\n",
    "if not os.path.exists(peaks_dataset_folder):\n",
    "    os.mkdir(peaks_dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_peaks = peaks_matched_table['unit_id'].value_counts()\n",
    "selected_peaks = selected_peaks[selected_peaks >= 500].index\n",
    "selected_peaks = selected_peaks.to_list()\n",
    "selected_peaks.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak_labels = ['unit_' + str(unit) for unit in selected_peaks]\n",
    "peak_labels_dict = {name: index for index, name in enumerate(peak_labels)}\n",
    "peaks_dataset = dss.TensorDataset(peaks_dataset_folder, peak_labels, peak_labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(peaks_dataset))\n",
    "test_size = len(peaks_dataset) - train_size\n",
    "peaks_train_dataset, peaks_test_dataset = torch.utils.data.random_split(peaks_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(peaks_train_dataset))\n",
    "print(\"Testing size:\",len(peaks_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataLoader instances for train and test datasets\n",
    "peaks_train_dataloader = DataLoader(\n",
    "    peaks_train_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    shuffle=True  # Shuffle the train dataset during training\n",
    ")\n",
    "\n",
    "peaks_test_dataloader = DataLoader(\n",
    "    peaks_test_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classify peaks with a CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1 = nn.Conv3d(1, 32, kernel_size=(9, 3, 2)) \n",
    "        self.conv_layer_2 = nn.Conv2d(32, 64, kernel_size=4) \n",
    "        \n",
    "        self.conv_layer_2_drop = nn.Dropout2d()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fully_connected_layer_1 = nn.Linear(35328, 500)\n",
    "        self.fully_connected_layer_2 = nn.Linear(500, 316)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(torch.squeeze(self.conv_layer_1(x), 4), 2))\n",
    "        \n",
    "        x = F.relu(F.max_pool2d(self.conv_layer_2_drop(self.conv_layer_2(x)), 2))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fully_connected_layer_1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fully_connected_layer_2(x))\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "model = CNNet().to(device)\n",
    "\n",
    "# Choose optimal parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(64, 1, 64, 192, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_folder = os.path.join(os.getcwd(), \"models\")\n",
    "\n",
    "if not os.path.exists(models_folder):\n",
    "    os.mkdir(models_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_peaks = dss.TrainModel(peaks_train_dataloader,\n",
    "                             peaks_test_dataloader,\n",
    "                             device,\n",
    "                             loss_fn,\n",
    "                             optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"peak_2\"\n",
    "\n",
    "train_peaks.train_test_model(model, model_name, models_folder, epochs=15, classes=selected_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpikeSortv2",
   "language": "python",
   "name": "spike_sort_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
