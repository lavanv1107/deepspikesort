{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: A Semi-Supervised Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract the data in an NWB file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import psutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "from pycave.bayes import GaussianMixture\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"SpikeInterface version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import process_peaks\n",
    "import dataset\n",
    "import model\n",
    "import clustering\n",
    "import training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_folder = Path(\".\")\n",
    "nwb_file = \"sub-CSHL049_ses-c99d53e6-c317-4c53-99ba-070b26673ac4_behavior+ecephys+image.nwb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_nwb = si.read_nwb(nwb_file, electrical_series_name='ElectricalSeriesAp')\n",
    "recording_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_nwb.annotate(is_filtered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_nwb = si.read_nwb_sorting(file_path=nwb_file, electrical_series_name='ElectricalSeriesAp')\n",
    "sorting_nwb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_f = si.bandpass_filter(recording_nwb, freq_min=300, freq_max=6000)\n",
    "recording_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "recording_cmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract channels and spikes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_slice = preprocessing.channel_slice_electricalseriesap(recording_cmr)\n",
    "recording_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channels_table = preprocessing.extract_channels(recording_slice)\n",
    "display(channels_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_table['channel_loc_x'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waveform_folder = 'waveform'\n",
    "\n",
    "job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (base_folder / waveform_folder).is_dir():\n",
    "    waveform_nwb = si.load_waveforms(base_folder / waveform_folder, with_recording=False)\n",
    "else:\n",
    "    waveform_nwb = si.extract_waveforms(\n",
    "        recording_slice,\n",
    "        sorting_nwb,\n",
    "        waveform_folder,\n",
    "        ms_before=1.5,\n",
    "        ms_after=2.,\n",
    "        max_spikes_per_unit=None,\n",
    "        overwrite=True,\n",
    "        **job_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waveform_nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes_table = preprocessing.extract_spikes(sorting_nwb, waveform_nwb)\n",
    "display(spikes_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Match peaks to spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_folder = 'peaks'\n",
    "\n",
    "job_kwargs = dict(chunk_duration='1s', n_jobs=8, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (base_folder / peaks_folder).is_dir():\n",
    "    peaks = process_peaks.load_peaks(base_folder / peaks_folder)\n",
    "else:\n",
    "    peaks = detect_peaks(recording_slice,\n",
    "                         method='locally_exclusive',\n",
    "                         peak_sign='neg',\n",
    "                         detect_threshold=6,\n",
    "                         **job_kwargs\n",
    "                        )\n",
    "    process_peaks.save_peaks(peaks, base_folder / peaks_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_table = process_peaks.extract_peaks(recording_slice, peaks)\n",
    "display(peaks_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_matched_table_file = os.path.join(peaks_folder, \"peaks_matched_table.pkl\")\n",
    "\n",
    "if os.path.exists(peaks_matched_table_file):\n",
    "    peaks_matched_table = pd.read_pickle(peaks_matched_table_file)\n",
    "else:\n",
    "    peaks_matched_table = process_peaks.match_peaks(peaks_table, spikes_table, channels_table)\n",
    "    peaks_matched_table.to_pickle(peaks_matched_table_file)\n",
    "    \n",
    "display(peaks_matched_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_spikes_table = process_peaks.get_peaks_spikes(peaks_matched_table)\n",
    "peaks_noise_table = process_peaks.get_peaks_noise(peaks_matched_table)\n",
    "\n",
    "display(peaks_spikes_table)\n",
    "display(peaks_noise_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a dataset from matched peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_dataset_folder = os.path.join(peaks_folder, 'peaks_dataset')\n",
    "\n",
    "if not os.path.exists(peaks_dataset_folder):\n",
    "    os.mkdir(peaks_dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_peaks = peaks_matched_table['unit_id'].value_counts()\n",
    "selected_peaks = selected_peaks[(selected_peaks >= 1000) & (selected_peaks <= 3700)].index\n",
    "selected_peaks = selected_peaks.to_list()\n",
    "\n",
    "print(len(selected_peaks))\n",
    "print(selected_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_dataset = dataset.TensorDataset(peaks_dataset_folder, selected_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(peaks_dataset))\n",
    "test_size = len(peaks_dataset) - train_size\n",
    "peaks_train_dataset, peaks_test_dataset = torch.utils.data.random_split(peaks_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(peaks_train_dataset))\n",
    "print(\"Testing size:\",len(peaks_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peaks_train_dataloader = DataLoader(\n",
    "    peaks_train_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtaining cluster assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the extractor architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Extractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1 = nn.Conv3d(1, 32, kernel_size=(9, 3, 2)) \n",
    "        self.conv_layer_2 = nn.Conv2d(32, 64, kernel_size=4) \n",
    "        \n",
    "        self.conv_layer_2_drop = nn.Dropout2d()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fully_connected_layer_1 = nn.Linear(35328, 500)\n",
    "        \n",
    "        # Initialize the weights as per the provided AlexNet\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(torch.squeeze(self.conv_layer_1(x), 4), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv_layer_2_drop(self.conv_layer_2(x)), 2))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fully_connected_layer_1(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of model\n",
    "model = Extractor()\n",
    "summary(model, input_size=(64, 1, 64, 192, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_file = os.path.join(peaks_folder, \"features_100.npy\")\n",
    "folders_file = os.path.join(peaks_folder, \"folders_100.pkl\")\n",
    "\n",
    "if os.path.exists(features_file):\n",
    "    features = np.load(features_file, allow_pickle=True)\n",
    "    # Load folder list\n",
    "    with open(folders_file, 'rb') as f:\n",
    "        folders = pickle.load(f)\n",
    "else:\n",
    "    device_ids = [0, 1, 2, 3]\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    features, folders = clustering.extract_features(peaks_train_dataloader, model, device, device_ids)\n",
    "    \n",
    "    # Save features\n",
    "    np.save(features_file, features)\n",
    "    \n",
    "    # Save folder list\n",
    "    with open(folders_file, 'wb') as f:\n",
    "        pickle.dump(folders, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_features = clustering.preprocess_features(features, n_components=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(Counter(folders)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(100, covariance_type=\"full\", init_strategy='kmeans', trainer_params=dict(gpus=[0]))\n",
    "gmm.fit(preprocessed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_assignments = gmm.predict(preprocessed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_assignments = cluster_assignments.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning cluster representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reassigned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_cluster_map = {}\n",
    "\n",
    "for folder, cluster in zip(folders, cluster_assignments):\n",
    "    folder_to_cluster_map[folder] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(folder_to_cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_dataset = dataset.ClusteredDataset(\n",
    "    peaks_dataset_folder,\n",
    "    selected_peaks,\n",
    "    folder_to_cluster_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(peaks_dataset))\n",
    "test_size = len(peaks_dataset) - train_size\n",
    "clustered_train_dataset, clustered_test_dataset = torch.utils.data.random_split(clustered_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(clustered_train_dataset))\n",
    "print(\"Testing size:\",len(clustered_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataLoader instances for train and test datasets\n",
    "clustered_train_dataloader = DataLoader(\n",
    "    clustered_train_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "clustered_test_dataloader = DataLoader(\n",
    "    clustered_test_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the classifier architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1 = nn.Conv3d(1, 32, kernel_size=(9, 3, 2)) \n",
    "        self.conv_layer_2 = nn.Conv2d(32, 64, kernel_size=4) \n",
    "        \n",
    "        self.conv_layer_2_drop = nn.Dropout2d()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fully_connected_layer_1 = nn.Linear(35328, 500)\n",
    "        self.fully_connected_layer_2 = nn.Linear(500, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(torch.squeeze(self.conv_layer_1(x), 4), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv_layer_2_drop(self.conv_layer_2(x)), 2))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fully_connected_layer_1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fully_connected_layer_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of model\n",
    "classifier = Classifier()\n",
    "summary(classifier, input_size=(64, 1, 64, 192, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose optimal parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_folder = os.path.join(os.getcwd(), \"models\")\n",
    "\n",
    "if not os.path.exists(models_folder):\n",
    "    os.mkdir(models_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_clusters = training.TrainModel(clustered_train_dataloader,\n",
    "                                     clustered_test_dataloader,\n",
    "                                     device,\n",
    "                                     loss_fn,\n",
    "                                     optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\")\n",
    "classifier = classifier.to(device)\n",
    "model_name = \"classifier\"\n",
    "\n",
    "train_clusters.train_test_model(classifier, model_name, models_folder, epochs=1, classes=selected_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing DeepCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "dataset = importlib.reload(dataset)\n",
    "model = importlib.reload(model)\n",
    "clustering = importlib.reload(clustering)\n",
    "training = importlib.reload(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device_ids = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deep_cluster_model = model.DeepCluster(100)\n",
    "gmm = GaussianMixture(100, covariance_type=\"full\", init_strategy='kmeans', trainer_params=dict(gpus=[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_iterations = 3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(deep_cluster_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_folder = os.path.join(os.getcwd(), \"models\")\n",
    "\n",
    "if not os.path.exists(models_folder):\n",
    "    os.mkdir(models_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    print(f\"Iteration {iteration + 1}\")\n",
    "  \n",
    "    # Step 1: Feature Extraction\n",
    "    features, folders = clustering.extract_features(peaks_train_dataloader, 500, deep_cluster_model, device, device_ids) \n",
    "    preprocessed_features = clustering.preprocess_features(features, n_components=100) \n",
    "  \n",
    "    # Step 2: Clustering\n",
    "    gmm.fit(preprocessed_features)\n",
    "    cluster_assignments = gmm.predict(preprocessed_features)\n",
    "    cluster_assignments = cluster_assignments.cpu().numpy()\n",
    "  \n",
    "    # Step 3: Cluster Reassignment\n",
    "    folder_to_cluster_map = {folder: cluster for folder, cluster in zip(folders, cluster_assignments)}\n",
    "    clustered_dataset = dataset.ClusteredDataset(peaks_dataset_folder, selected_peaks, folder_to_cluster_map)  \n",
    "    train_size = int(0.7 * len(clustered_dataset))\n",
    "    test_size = len(clustered_dataset) - train_size\n",
    "    clustered_train_dataset, clustered_test_dataset = torch.utils.data.random_split(clustered_dataset, [train_size, test_size])\n",
    "  \n",
    "    clustered_train_dataloader = DataLoader(clustered_train_dataset, batch_size=8)\n",
    "    clustered_test_dataloader = DataLoader(clustered_test_dataset, batch_size=8)\n",
    "  \n",
    "    # Step 4: Classification\n",
    "    train_clusters = training.TrainModel(clustered_train_dataloader, clustered_test_dataloader, device, device_ids, loss_fn, optimizer)\n",
    "    train_clusters.train_test_model(deep_cluster_model, \"sup_dss_1\", models_folder, epochs=1, classes=selected_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSpikeSort",
   "language": "python",
   "name": "deep_spike_sort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "025e200be8744c8c99be5c5fadfcaed6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "06b74d374b264ab78e8868efa1997da4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "16382ad09068474786ca5e612510e136": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "16e6eefcd15e429ab5bf6fb773e6efa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2da43649f1834473a4bd1f56dea2ba73",
       "style": "IPY_MODEL_025e200be8744c8c99be5c5fadfcaed6",
       "value": "Epoch 198: 100%"
      }
     },
     "17a7eb0c43194aabaaceae264c87d357": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_582719d0e21f491cbaa73ce9e486fcb3",
       "style": "IPY_MODEL_477baa69459b40449fb1d8e4179e9fc7",
       "value": "Epoch 271: 100%"
      }
     },
     "1b5a1017149e497e9eec4d2ad794c76b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "2da43649f1834473a4bd1f56dea2ba73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2eb60f9b09fb47f9b3a2e5a0e3fe7655": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3698421009cf43c994885d242294dfd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_62ef8de2dd1e416b8b18a14dea60a233",
       "max": 1,
       "style": "IPY_MODEL_fe9194b4f41c449ea0190fe805a96e24"
      }
     },
     "402060110f1f4b6d85291d10b7191a8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_76154f3321454557ba373b5367ccfd58",
       "style": "IPY_MODEL_ed5e423bb8ca4c50bd0e568363a2e394",
       "value": " 1/1 [00:00&lt;00:00, 369.51it/s]"
      }
     },
     "477baa69459b40449fb1d8e4179e9fc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4e86a7902d574dc4bb6da5e97ef73ffb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "542fafa4e8604f4bb601dbe06bf1743d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_abb5745b4ea1435fb532a86048b4f084",
       "max": 1,
       "style": "IPY_MODEL_7419b02064764d86bea0866cac9b4357",
       "value": 1
      }
     },
     "582719d0e21f491cbaa73ce9e486fcb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "62ef8de2dd1e416b8b18a14dea60a233": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "7419b02064764d86bea0866cac9b4357": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "76154f3321454557ba373b5367ccfd58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "841100d26c4a42bcbe065667891da100": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "85c9c24d2b954f9195e61de6062dcca4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_17a7eb0c43194aabaaceae264c87d357",
        "IPY_MODEL_3698421009cf43c994885d242294dfd5",
        "IPY_MODEL_f8ac28ce078a438b8ff614b2276edf94"
       ],
       "layout": "IPY_MODEL_841100d26c4a42bcbe065667891da100"
      }
     },
     "8e3dc99307da4291b1f0b9997b09cad5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_16e6eefcd15e429ab5bf6fb773e6efa8",
        "IPY_MODEL_542fafa4e8604f4bb601dbe06bf1743d",
        "IPY_MODEL_bf6df6bc1b7142fc9dd0725064484e23"
       ],
       "layout": "IPY_MODEL_16382ad09068474786ca5e612510e136"
      }
     },
     "abb5745b4ea1435fb532a86048b4f084": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "b19e0334e6d74adbab84cd11a610d3e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_1b5a1017149e497e9eec4d2ad794c76b",
       "max": 1,
       "style": "IPY_MODEL_c2117759204343358ae209e1317d0a99",
       "value": 1
      }
     },
     "b88e1f7bff8c41928a2a2b3fb7aa10c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ed415406f57944c5ac9f251720ff8c2f",
        "IPY_MODEL_b19e0334e6d74adbab84cd11a610d3e7",
        "IPY_MODEL_402060110f1f4b6d85291d10b7191a8b"
       ],
       "layout": "IPY_MODEL_4e86a7902d574dc4bb6da5e97ef73ffb"
      }
     },
     "bf6df6bc1b7142fc9dd0725064484e23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2eb60f9b09fb47f9b3a2e5a0e3fe7655",
       "style": "IPY_MODEL_ec55266d32c4441583a672bbebf5623f",
       "value": " 1/1 [00:00&lt;00:00, 347.61it/s]"
      }
     },
     "c2117759204343358ae209e1317d0a99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d66bb56722904ffb8c01702c98be63e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dd022a8af0904d5fb0adc56391f10dea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ebd8138c72414741b9a68594dd26a7fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec55266d32c4441583a672bbebf5623f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ed415406f57944c5ac9f251720ff8c2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ebd8138c72414741b9a68594dd26a7fb",
       "style": "IPY_MODEL_06b74d374b264ab78e8868efa1997da4",
       "value": "Epoch 299: 100%"
      }
     },
     "ed5e423bb8ca4c50bd0e568363a2e394": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f8ac28ce078a438b8ff614b2276edf94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dd022a8af0904d5fb0adc56391f10dea",
       "style": "IPY_MODEL_d66bb56722904ffb8c01702c98be63e9",
       "value": " 1/1 [00:00&lt;00:00, 313.16it/s, inertia=0.902]"
      }
     },
     "fe9194b4f41c449ea0190fe805a96e24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
