{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "from spikeinterface.sortingcomponents.peak_detection import detect_peaks\n",
    "\n",
    "print(f\"SpikeInterface version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import comparison\n",
    "import create_dataset.process_peaks\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import plotting\n",
    "import preprocessing\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract peaks from recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "print(h5py.__version__)\n",
    "print(h5py.version.hdf5_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data/sub-CSHL049\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Create a dataset from peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Match peaks to spikes - this is used only for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes_file = os.path.join(data_folder, \"spikes/spikes.npy\")\n",
    "spikes = np.load(spikes_file)\n",
    "\n",
    "peaks_folder = os.path.join(data_folder, \"peaks\")\n",
    "peaks_file = os.path.join(peaks_folder, \"peaks.npy\")\n",
    "peaks_filtered = np.load(peaks_file)\n",
    "\n",
    "peaks_matched_file = os.path.join(peaks_folder, \"peaks_matched.npy\")\n",
    "if os.path.exists(peaks_matched_file):\n",
    "    peaks_matched = np.load(peaks_matched_file)\n",
    "else:\n",
    "    peaks_matched = process_peaks.match_peaks(peaks_filtered, spikes)\n",
    "    np.save(peaks_matched_file, peaks_matched)\n",
    "    \n",
    "display(pd.DataFrame(peaks_matched))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run DeepSpikeSort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepSpikeSort algorithm can be run using the `run_dss.py` script.\n",
    "\n",
    "DeepSpikeSort or DSS follows the DeepCluster method using the following steps:\n",
    "\n",
    "1. Feature Extraction\n",
    "- Initialize the CNN model with random weights for the first epoch\n",
    "- Extract features before the final FC layer\n",
    "- Preprocess features using PCA, whitening and l2-normalization\n",
    "\n",
    "2. Clustering\n",
    "- Fit a GMM with the preprocessed features \n",
    "- Predict cluster labels for the features\n",
    "\n",
    "3. Cluster Comparison\n",
    "- Calculate the ARI (Adjusted Rand Index) between epochs after the first epoch\n",
    "- Set the ARI value as a metric for convergence\n",
    "\n",
    "4. Representation Learning\n",
    "- Create a dataset using the cluster labels for supervised learning\n",
    "- Train the CNN model with labelled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script is currently set up to select the peaks corresponding to matched units that satisfy the requirements (e.g., minimum number of samples).\n",
    "\n",
    "This script requires `data/{recording_id}/peaks/peaks_matched.npy` and `data/{recording_id}/channels.npy`\n",
    "\n",
    "Example: `!python run_dss.py --recording_id=sub-CSHL049 --num_units=5 --min_samples=3000 --max_samples=4000 --seed=0 --num_samples=all --noise_samples=0 method=mask trial_name=dss trial_number=0`\n",
    "\n",
    "The example command will run DSS:\n",
    "- using recording sub-CSHL049\n",
    "- on 5 units\n",
    "- with 3000-4000 samples per unit\n",
    "- ...\n",
    "\n",
    "The script will also save the DSS output and results to their respective folders:\n",
    "- Output\n",
    "    - Selected units\n",
    "    - Preprocessed features\n",
    "    - Cluster labels\n",
    "    - Corresponding times\n",
    "- Results\n",
    "    - ARI progress plot\n",
    "    - ARI progress log\n",
    "    - SpikeInterface comparison results\n",
    "    - Agreement matrix plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Inspect clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"output/sub-CSHL049\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def parse_log_file(file_path: str) -> List[Tuple[int, float, float]]:\n",
    "    \"\"\"\n",
    "    Parse the log file and extract epoch, loss, and accuracy values.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the log file.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[int, float, float]]: List of tuples containing (epoch, loss, accuracy).\n",
    "    \"\"\"\n",
    "    pattern = r'\\[(\\d+)\\]\\s+Loss:\\s+([\\d.]+)\\s+Accuracy:\\s+([\\d.]+)'\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                epoch = int(match.group(1))\n",
    "                loss = float(match.group(2))\n",
    "                accuracy = float(match.group(3))\n",
    "                data.append((epoch, loss, accuracy))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_plot(data: List[Tuple[int, float, float]]) -> None:\n",
    "    \"\"\"\n",
    "    Create a plot showing loss and accuracy over epochs in separate subplots.\n",
    "    \n",
    "    Args:\n",
    "        data (List[Tuple[int, float, float]]): List of tuples containing (epoch, loss, accuracy).\n",
    "    \"\"\"\n",
    "    epochs, losses, accuracies = zip(*data)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(epochs, losses, color='tab:red', label='Loss')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(epochs, accuracies, color='tab:blue', label='Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_date = '2024-08-23'\n",
    "trial_id = 'DSS_002'\n",
    "\n",
    "log_file_path = os.path.join(output_folder, f'{session_date}/{trial_id}/{trial_id}_performance_metrics.log')\n",
    "data = parse_log_file(log_file_path)\n",
    "create_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_date = '2024-08-23'\n",
    "trial_id = 'DSS_000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "previous_labels = np.load(os.path.join(output_folder, f'{session_date}/{trial_id}/previous_labels.npy'))\n",
    "labels_current = np.load(os.path.join(output_folder, f'{session_date}/{trial_id}/current_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_sorting = si.NpzSortingExtractor(os.path.join(output_folder, f'{session_date}/{trial_id}/current_sorting.npz'))\n",
    "previous_sorting = si.NpzSortingExtractor(os.path.join(output_folder, f'{session_date}/{trial_id}/previous_sorting.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison_cluster = si.compare_two_sorters(\n",
    "            sorting1=current_sorting,\n",
    "            sorting2=previous_sorting,\n",
    "            sorting1_name=\"Current\",\n",
    "            sorting2_name=\"Previous\",\n",
    "            delta_time=0,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "si.plot_agreement_matrix(sorting_comparison=comparison_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relabel_clusters(previous_sorting, current_sorting, cluster_labels):\n",
    "    # Compare the two sortings\n",
    "    comparison = si.compare_two_sorters(\n",
    "        sorting1=current_sorting,\n",
    "        sorting2=previous_sorting,\n",
    "        sorting1_name=\"Current\",\n",
    "        sorting2_name=\"Previous\",\n",
    "        delta_time=0,\n",
    "        verbose=True  \n",
    "    )\n",
    "    \n",
    "    # Get the matching between current and previous labels\n",
    "    matching = comparison.get_matching()[0]\n",
    "    \n",
    "    # Find missing labels from the previous set\n",
    "    all_possible_labels = set(range(len(matching)))\n",
    "    used_labels = set(label for label in matching if label != -1)\n",
    "    missing_labels = list(all_possible_labels - used_labels)\n",
    "    \n",
    "    # Randomly shuffle the missing labels\n",
    "    np.random.shuffle(missing_labels)\n",
    "    \n",
    "    # Replace -1 values with randomly selected missing labels\n",
    "    for i in range(len(matching)):\n",
    "        if matching[i] == -1:\n",
    "            if missing_labels:\n",
    "                matching[i] = missing_labels.pop(0)\n",
    "    \n",
    "    # Create the mapping\n",
    "    label_map = {i: int(matching[i]) for i in range(len(matching))}\n",
    "    \n",
    "    # Apply the mapping to cluster_labels\n",
    "    new_labels = np.array([label_map[label] for label in cluster_labels], dtype=int)\n",
    "    \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relabel_clusters_add(previous_sorting, current_sorting, cluster_labels):\n",
    "    # Compare the two sortings\n",
    "    comparison = si.compare_two_sorters(\n",
    "        sorting1=current_sorting,\n",
    "        sorting2=previous_sorting,\n",
    "        sorting1_name=\"Current\",\n",
    "        sorting2_name=\"Previous\",\n",
    "        delta_time=0,\n",
    "        verbose=False  # Set to True for debugging\n",
    "    )\n",
    "\n",
    "    # Get the matching between current and previous labels\n",
    "    matching = comparison.get_matching()[0].astype(int)\n",
    "\n",
    "    # Create a mapping dictionary, including -1 values for unmatched clusters\n",
    "    labels_map = {int(current): int(previous) for current, previous in matching.items()}\n",
    "\n",
    "    # Handle unmatched clusters\n",
    "    current_labels = np.array(list(labels_map.keys()))\n",
    "    previous_labels = np.array(list(labels_map.values()))\n",
    "\n",
    "    # Find the maximum label used\n",
    "    max_label = max(max(current_labels), max(previous_labels[previous_labels != -1]))\n",
    "\n",
    "    # Create a new mapping\n",
    "    new_map = {}\n",
    "    next_new_label = max_label + 1\n",
    "\n",
    "    for current, previous in labels_map.items():\n",
    "        if previous == -1:\n",
    "            new_map[current] = next_new_label\n",
    "            next_new_label += 1\n",
    "        else:\n",
    "            new_map[current] = previous\n",
    "\n",
    "    # Create a mapping array\n",
    "    max_label = max(max(cluster_labels), max(new_map.values()))\n",
    "    map_array = np.arange(max_label + 1)  # Default to identity mapping\n",
    "    for current, previous in new_map.items():\n",
    "        map_array[current] = previous\n",
    "\n",
    "    # Apply the mapping to cluster_labels\n",
    "    new_labels = map_array[cluster_labels]\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_labels = relabel_clusters(previous_sorting, current_sorting, labels_current)\n",
    "np.unique(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare the two sortings\n",
    "comparison = si.compare_two_sorters(\n",
    "    sorting1=current_sorting,\n",
    "    sorting2=previous_sorting,\n",
    "    sorting1_name=\"Current\",\n",
    "    sorting2_name=\"Previous\",\n",
    "    delta_time=0,\n",
    "    verbose=True  \n",
    ")\n",
    "    \n",
    "# Get the matching between current and previous labels\n",
    "matching = comparison.get_matching()[0].astype(int)\n",
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_id = 'DSS_000'\n",
    "session_date = '2024-08-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = np.load(os.path.join(output_folder, f'{session_date}/{trial_id}/features_before_pca.npy'))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_zero_features(features):\n",
    "    # Sum along the first axis (across all samples)\n",
    "    feature_sums = np.sum(features, axis=0)\n",
    "    \n",
    "    # Find indices where the sum is zero\n",
    "    zero_features = np.where(feature_sums == 0)[0]\n",
    "    \n",
    "    return zero_features\n",
    "\n",
    "# Check for zero features\n",
    "zero_feature_indices = check_zero_features(features_normalized)\n",
    "\n",
    "# Print results\n",
    "if len(zero_feature_indices) > 0:\n",
    "    print(f\"Found {len(zero_feature_indices)} features with all zero values.\")\n",
    "    print(\"Indices of all-zero features:\", zero_feature_indices)\n",
    "else:\n",
    "    print(\"No features with all zero values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_nan_values(features):\n",
    "    # Check for NaN values\n",
    "    nan_mask = np.isnan(features)\n",
    "    \n",
    "    # Find features with any NaN values\n",
    "    features_with_nan = np.any(nan_mask, axis=0)\n",
    "    \n",
    "    # Get indices of features with NaN values\n",
    "    nan_feature_indices = np.where(features_with_nan)[0]\n",
    "    \n",
    "    # Count NaN values per feature\n",
    "    nan_counts = np.sum(nan_mask, axis=0)\n",
    "    \n",
    "    return nan_feature_indices, nan_counts[nan_feature_indices]\n",
    "\n",
    "# Check for NaN values\n",
    "nan_indices, nan_counts = check_nan_values(features_normalized)\n",
    "\n",
    "# Print results\n",
    "if len(nan_indices) > 0:\n",
    "    print(f\"Found {len(nan_indices)} features with NaN values.\")\n",
    "    for idx, count in zip(nan_indices, nan_counts):\n",
    "        print(f\"Feature {idx}: {count} NaN values\")\n",
    "else:\n",
    "    print(\"No NaN values found in the array.\")\n",
    "\n",
    "# Get total number of NaN values\n",
    "total_nans = np.sum(nan_counts)\n",
    "print(f\"Total number of NaN values in the array: {total_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "\n",
    "# Calculate cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the number of components that explain 95% of the variance\n",
    "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"Number of components explaining 95% of variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform PCA with 50 components\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(features)\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot 1: Cumulative Explained Variance\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 51), cumulative_variance_ratio, 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Components')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Individual Explained Variance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(1, 51), explained_variance_ratio)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio per Principal Component')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the total explained variance with 50 components\n",
    "print(f\"Total explained variance with 50 components: {cumulative_variance_ratio[-1]:.4f}\")\n",
    "\n",
    "# Find the number of components needed to explain 95% of the variance\n",
    "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"Number of components explaining 95% of variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def calculate_cluster_separation(X_pca, min_components, max_components):\n",
    "    silhouette_scores = []\n",
    "    all_cluster_labels = []\n",
    "    n_components_range = range(min_components, max_components)\n",
    "    for n_components in n_components_range:\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        \n",
    "        # Fit the GMM and predict cluster labels\n",
    "        cluster_labels = gmm.fit_predict(X_pca[:, :4])\n",
    "        all_cluster_labels.append(cluster_labels)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(X_pca[:, :4], cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        \n",
    "        print(f\"For n_components = {n_components}, the average silhouette score is : {silhouette_avg}\")\n",
    "    \n",
    "    best_n_components = n_components_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"\\nThe best number of components appears to be {best_n_components}\")\n",
    "    \n",
    "    return silhouette_scores, best_n_components, all_cluster_labels\n",
    "\n",
    "# silhouette_scores, best_n_clusters, cluster_labels = calculate_cluster_separation(X_pca, 9, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "# Create a colormap with 5 distinct colors\n",
    "colors = ['red', 'yellow', 'green', 'blue', 'purple']\n",
    "n_bins = 5  # number of distinct colors\n",
    "cmap = plt.cm.colors.ListedColormap(colors)\n",
    "    \n",
    "def plot_3d_pca(X_pca, labels=None):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap=cmap)\n",
    "    \n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    ax.set_title('First Three Principal Components')\n",
    "    \n",
    "    if labels is not None:\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_3d_pca(X_pca[:, :3], labels=cluster_labels)  # cluster_labels from KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the first 4 principal components\n",
    "first_four_pcs = X_pca[:, :4]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.subplots_adjust(right=0.85)  # Make room for the colorbar\n",
    "axs = axs.ravel()  # Flatten the 2D array of axes for easier indexing\n",
    "\n",
    "# Plot each pair of principal components\n",
    "pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "for idx, (i, j) in enumerate(pairs):\n",
    "    scatter = axs[idx].scatter(first_four_pcs[:, i], first_four_pcs[:, j], \n",
    "                               c=cluster_labels, cmap=cmap, alpha=0.6)\n",
    "    axs[idx].set_xlabel(f'PC{i+1}')\n",
    "    axs[idx].set_ylabel(f'PC{j+1}')\n",
    "    axs[idx].set_title(f'PC{i+1} vs PC{j+1}')\n",
    "\n",
    "# Add a colorbar to the right of the subplots\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "cbar.set_label('Cluster')\n",
    "\n",
    "plt.suptitle('Pairwise PCA Component Plots with Cluster Coloring', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 0.85, 0.95])  # Adjust layout to accommodate suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_date = '2024-08-23'\n",
    "trial_id = 'DSS_001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = os.path.join(output_folder, f'{session_date}/{trial_id}/{trial_id}_output_data.h5')\n",
    "\n",
    "with h5py.File(hdf5_file, 'r') as handle:\n",
    "    features = handle['features'][:]\n",
    "    labels = handle['labels'][:]\n",
    "    properties = handle['properties'][:]\n",
    "    metrics = handle['metrics'][:]\n",
    "    \n",
    "epoch = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_3d_pca(X_pca, labels=None):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='rainbow')\n",
    "    \n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    ax.set_title('First Three Principal Components')\n",
    "    \n",
    "    if labels is not None:\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_3d_pca(features[epoch, :, :3], labels=labels[epoch, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the first 4 principal components\n",
    "first_four_pcs = features[epoch, :, :4]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.subplots_adjust(right=0.85)  # Make room for the colorbar\n",
    "axs = axs.ravel()  # Flatten the 2D array of axes for easier indexing\n",
    "\n",
    "# Plot each pair of principal components\n",
    "pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "for idx, (i, j) in enumerate(pairs):\n",
    "    scatter = axs[idx].scatter(first_four_pcs[:, i], first_four_pcs[:, j], \n",
    "                               c=labels[epoch, :], cmap='rainbow', alpha=0.6)\n",
    "    axs[idx].set_xlabel(f'PC{i+1}')\n",
    "    axs[idx].set_ylabel(f'PC{j+1}')\n",
    "    axs[idx].set_title(f'PC{i+1} vs PC{j+1}')\n",
    "\n",
    "# Add a colorbar to the right of the subplots\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "cbar.set_label('Cluster')\n",
    "\n",
    "plt.suptitle('Pairwise PCA Component Plots with Cluster Coloring', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 0.85, 0.95])  # Adjust layout to accommodate suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation.animate_plot(evaluation.plot_cluster_distribution, plot_args=(labels,), epoch_start=76, epoch_end=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation.animate_plot(evaluation.plot_variance, plot_args=(features,20), epoch_start=90, epoch_end=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation.plot_epoch_clusters(features, labels, 50, num_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Compare DeepSpikeSort output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Sorting object from DSS output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dss_output = np.zeros(len(labels[epoch]), dtype=[('peak_index', int), ('sample_index', int), ('channel_index', int), ('amplitude', int), ('unit_index', int)])\n",
    "\n",
    "dss_output['peak_index'] = properties['peak_index']\n",
    "dss_output['sample_index'] = properties['sample_index']\n",
    "dss_output['channel_index'] = properties['channel_index']\n",
    "dss_output['amplitude'] = properties['amplitude']\n",
    "dss_output['unit_index'] = labels[epoch]\n",
    "\n",
    "display(pd.DataFrame(dss_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dss_filtered = comparison.filter_samples_duplicate(dss_output)\n",
    "dss_times = dss_output['sample_index']\n",
    "dss_labels = dss_output['unit_index']\n",
    "\n",
    "print(f'Samples: {len(dss_labels)}\\n')\n",
    "print(util.format_value_counts(dss_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create custom NumpySorting object from DeepSpikeSort output\n",
    "sorting_dss = comparison.create_numpy_sorting(dss_times, dss_labels, 30000)\n",
    "sorting_dss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sorting object from NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a boolean mask\n",
    "units_selected_file = os.path.join(output_folder, f'{session_date}/{trial_id}/{trial_id}_units_selected.npy')\n",
    "# units_selected_file = os.path.join(output_folder, f'{session_date}/{trial_id}/{trial_id}_selected_units.npy')\n",
    "units_selected = np.load(units_selected_file).astype(int)\n",
    "\n",
    "mask_selected = np.isin(peaks_matched['unit_index'], units_selected)\n",
    "\n",
    "# Filter the array\n",
    "peaks_selected = peaks_matched[mask_selected]\n",
    "display(pd.DataFrame(peaks_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# peaks_filtered = comparison.filter_samples_duplicate(peaks_selected)\n",
    "peak_times = peaks_selected['time']\n",
    "peak_units = peaks_selected['unit_index']\n",
    "\n",
    "print(f'Samples: {len(peak_units)}\\n')\n",
    "print(util.format_value_counts(peak_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_peaks = comparison.create_numpy_sorting(peak_times, peak_units, 30000)\n",
    "sorting_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Sorting objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the comparison\n",
    "cmp_dss_peaks = si.compare_two_sorters(\n",
    "    sorting1=sorting_peaks,\n",
    "    sorting2=sorting_dss,\n",
    "    sorting1_name='DeepSpikeSort',\n",
    "    sorting2_name='Kilosort',\n",
    "    delta_time=0,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In order to check which units were matched, the `get_matching` method can be used.\n",
    "# If units are not matched they are listed as -1.\n",
    "dss_to_peaks = cmp_dss_peaks.get_matching()[1]\n",
    "display(dss_to_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some useful internal dataframes help to check the match and count\n",
    "#  like **match_event_count** or **agreement_scores**\n",
    "display(cmp_dss_peaks.match_event_count)\n",
    "display(cmp_dss_peaks.agreement_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can check the agreement matrix to inspect the matching.\n",
    "si.plot_agreement_matrix(cmp_dss_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sum(np.sum(cmp_dss_peaks.match_event_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Inspect mismatched peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_label = 0\n",
    "dss_output_cluster = np.sort(preprocessing.get_unit(dss_output, cluster_label), order='peak_index')\n",
    "\n",
    "pd.DataFrame(dss_output_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicate_peaks = evaluation.get_duplicate_peaks(dss_output_cluster)\n",
    "\n",
    "pd.DataFrame(duplicate_peaks[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_ind = 50\n",
    "neighbor_channels = np.sort(np.append(preprocessing.get_channel_neighbors(channels, channel_ind, 80)['channel_index'], channel_ind))\n",
    "\n",
    "neighbor_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_trace_waveform(recording_preprocessed, dss_output_cluster['sample_index'][0], neighbor_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_unit_waveform(recording_preprocessed, dss_output_cluster, cluster_label, 361, False, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unit_map = {idx: val for idx, val in enumerate(dss_to_peaks)}\n",
    "\n",
    "labels_st1, labels_st2 = si.do_score_labels(sorting_dss, sorting_peaks, 0, unit_map)\n",
    "print(labels_st1)\n",
    "print(labels_st2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dss_scores = comparison.get_scores(labels_st1, [0,1,2])\n",
    "pd.DataFrame(dss_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss_matched, dss_mismatched = comparison.filter_samples_on_match(labels_st1, [0,1,2], dss_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matched samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_label = 0\n",
    "dss_matched_cluster = np.sort(preprocessing.get_unit(dss_matched, cluster_label), order='unit_index')\n",
    "pd.DataFrame(dss_matched_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_index = 0\n",
    "trace_index = dss_matched_cluster['index'][row_index]\n",
    "trace_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_ind = peaks_matched['channel_index'][peaks_matched['sample_index'] == times[trace_index]] \n",
    "channel_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_unit_waveform(recording_preprocessed, dss_matched_cluster, cluster_label, channel_ind[0], False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_trace_image(trace_reshaped):\n",
    "    \"\"\"\n",
    "    Plots a 3D image of waveforms at the specified time frame and all channels.\n",
    " \n",
    "    Args:\n",
    "        recording (obj): A RecordingExtractor object created from an NWB file using SpikeInterface.\n",
    "        sample_frame (int): A frame number when a sample occurred.\n",
    " \n",
    "    Yields:\n",
    "        obj: A 3D image of waveforms.\n",
    "    \"\"\"\n",
    "    trace_transposed = np.transpose(trace_reshaped, (1, 0, 2))\n",
    "\n",
    "    vmin = trace_transposed.min()\n",
    "    vmax = trace_transposed.max()\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    for i in range(trace_reshaped.shape[2]):\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.imshow(trace_transposed[:, :, i], cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    # Set x and y labels for the plot\n",
    "    plt.text(0.5, 0.05, 'time (frames)', ha='center', va='center', transform=plt.gcf().transFigure)\n",
    "    plt.text(0.01, 0.5, 'channel', ha='center', va='center', rotation='vertical', transform=plt.gcf().transFigure)\n",
    "    # Add colorbar for the plot\n",
    "    cax = plt.axes([0.15, 0.95, 0.7, 0.03])  # [left, bottom, width, height]\n",
    "    cb = plt.colorbar(cax=cax, orientation='horizontal')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_trace_waveform(recording_preprocessed, dss_matched_cluster['sample_index'][:9], channel_ind[0])\n",
    "plot_trace_image(peaks_dataset[trace_index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mismatched samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_label = 1\n",
    "dss_mismatched_cluster = preprocessing.get_unit(dss_mismatched, cluster_label)\n",
    "pd.DataFrame(dss_mismatched_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_index = 0\n",
    "trace_index = dss_mismatched_cluster['index'][row_index]\n",
    "trace_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_ind = peaks_matched['channel_index'][peaks_matched['sample_index'] == times[trace_index]]\n",
    "channel_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_neighbors = preprocessing.get_channel_neighbors(channels, channel_ind[1], 40)['channel_index']\n",
    "channel_neighbors = np.sort(np.append(channel_neighbors, channel_ind[1]))\n",
    "channel_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_trace_waveform(recording_preprocessed, dss_mismatched_cluster['sample_index'][row_index], channel_neighbors)\n",
    "plot_trace_image(peaks_dataset[trace_index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from isosplit6 import isosplit6\n",
    "\n",
    "labels = isosplit6(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dss",
   "language": "python",
   "name": "dss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "025e200be8744c8c99be5c5fadfcaed6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "06b74d374b264ab78e8868efa1997da4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "16382ad09068474786ca5e612510e136": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "16e6eefcd15e429ab5bf6fb773e6efa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2da43649f1834473a4bd1f56dea2ba73",
       "style": "IPY_MODEL_025e200be8744c8c99be5c5fadfcaed6",
       "value": "Epoch 198: 100%"
      }
     },
     "17a7eb0c43194aabaaceae264c87d357": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_582719d0e21f491cbaa73ce9e486fcb3",
       "style": "IPY_MODEL_477baa69459b40449fb1d8e4179e9fc7",
       "value": "Epoch 271: 100%"
      }
     },
     "1b5a1017149e497e9eec4d2ad794c76b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "2da43649f1834473a4bd1f56dea2ba73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2eb60f9b09fb47f9b3a2e5a0e3fe7655": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3698421009cf43c994885d242294dfd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_62ef8de2dd1e416b8b18a14dea60a233",
       "max": 1,
       "style": "IPY_MODEL_fe9194b4f41c449ea0190fe805a96e24"
      }
     },
     "402060110f1f4b6d85291d10b7191a8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_76154f3321454557ba373b5367ccfd58",
       "style": "IPY_MODEL_ed5e423bb8ca4c50bd0e568363a2e394",
       "value": " 1/1 [00:00&lt;00:00, 369.51it/s]"
      }
     },
     "477baa69459b40449fb1d8e4179e9fc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4e86a7902d574dc4bb6da5e97ef73ffb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "542fafa4e8604f4bb601dbe06bf1743d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_abb5745b4ea1435fb532a86048b4f084",
       "max": 1,
       "style": "IPY_MODEL_7419b02064764d86bea0866cac9b4357",
       "value": 1
      }
     },
     "582719d0e21f491cbaa73ce9e486fcb3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "62ef8de2dd1e416b8b18a14dea60a233": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "7419b02064764d86bea0866cac9b4357": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "76154f3321454557ba373b5367ccfd58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "841100d26c4a42bcbe065667891da100": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "85c9c24d2b954f9195e61de6062dcca4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_17a7eb0c43194aabaaceae264c87d357",
        "IPY_MODEL_3698421009cf43c994885d242294dfd5",
        "IPY_MODEL_f8ac28ce078a438b8ff614b2276edf94"
       ],
       "layout": "IPY_MODEL_841100d26c4a42bcbe065667891da100"
      }
     },
     "8e3dc99307da4291b1f0b9997b09cad5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_16e6eefcd15e429ab5bf6fb773e6efa8",
        "IPY_MODEL_542fafa4e8604f4bb601dbe06bf1743d",
        "IPY_MODEL_bf6df6bc1b7142fc9dd0725064484e23"
       ],
       "layout": "IPY_MODEL_16382ad09068474786ca5e612510e136"
      }
     },
     "abb5745b4ea1435fb532a86048b4f084": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "b19e0334e6d74adbab84cd11a610d3e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_1b5a1017149e497e9eec4d2ad794c76b",
       "max": 1,
       "style": "IPY_MODEL_c2117759204343358ae209e1317d0a99",
       "value": 1
      }
     },
     "b88e1f7bff8c41928a2a2b3fb7aa10c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ed415406f57944c5ac9f251720ff8c2f",
        "IPY_MODEL_b19e0334e6d74adbab84cd11a610d3e7",
        "IPY_MODEL_402060110f1f4b6d85291d10b7191a8b"
       ],
       "layout": "IPY_MODEL_4e86a7902d574dc4bb6da5e97ef73ffb"
      }
     },
     "bf6df6bc1b7142fc9dd0725064484e23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2eb60f9b09fb47f9b3a2e5a0e3fe7655",
       "style": "IPY_MODEL_ec55266d32c4441583a672bbebf5623f",
       "value": " 1/1 [00:00&lt;00:00, 347.61it/s]"
      }
     },
     "c2117759204343358ae209e1317d0a99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d66bb56722904ffb8c01702c98be63e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dd022a8af0904d5fb0adc56391f10dea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ebd8138c72414741b9a68594dd26a7fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec55266d32c4441583a672bbebf5623f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ed415406f57944c5ac9f251720ff8c2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ebd8138c72414741b9a68594dd26a7fb",
       "style": "IPY_MODEL_06b74d374b264ab78e8868efa1997da4",
       "value": "Epoch 299: 100%"
      }
     },
     "ed5e423bb8ca4c50bd0e568363a2e394": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f8ac28ce078a438b8ff614b2276edf94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dd022a8af0904d5fb0adc56391f10dea",
       "style": "IPY_MODEL_d66bb56722904ffb8c01702c98be63e9",
       "value": " 1/1 [00:00&lt;00:00, 313.16it/s, inertia=0.902]"
      }
     },
     "fe9194b4f41c449ea0190fe805a96e24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
